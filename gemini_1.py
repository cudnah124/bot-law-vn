# rag_with_gemini.py
from google import genai
import json
import numpy as np
import faiss
from typing import List
import time
from dotenv import load_dotenv
import os

load_dotenv()
MAX_CHARS = 2000
api_key = os.getenv("GENAI_API_KEY")
def truncate_prompt(prompt: str):
    if len(prompt) > MAX_CHARS:
        return prompt[:MAX_CHARS] + "..."
    return prompt

#Client
client = genai.Client(api_key = api_key)  

# Data
with open("data.json", "r", encoding="utf-8") as f:
    docs = json.load(f)  # list of {id, dialect, text, norm, ...}

with open("law_data.json", "r", encoding="utf-8") as f:
    laws = json.load(f)  # list of {Source, text}

# padding
for law in laws:
    law["dialect"] = law["Source"]  
    law["meaning"] = ""             
    law["is_law"] = True

# padding
for d in docs:
    d["is_law"] = False

# 
all_data = docs + laws

texts = []
for d in all_data:
    if d.get("is_law", False):
        # Embed
        texts.append(f"Law Source: {d['Source']} ||| Text: {d['text']} ||| Type: {d.get('Type','')}")
    else:
        # Embed
        texts.append(f"Dialect: {d['dialect']} ||| Text: {d['text']} ||| Meaning: {d.get('meaning','')}")


# 3) embeddings (batch)
def embed_texts(text_list: List[str], batch_size=100):
    embeddings = []
    for i in range(0, len(text_list), batch_size):
        batch = text_list[i:i+batch_size]
        res = client.models.embed_content(
            model="models/embedding-001",
            contents=batch
        )
        embeddings.extend([e.values for e in res.embeddings])
    return embeddings

embs = embed_texts(texts)

# 4) build FAISS index (L2 / cosine -> normalize for cosine)
d = len(embs[0])
arr = np.array(embs).astype("float32")
# normalize for cosine similarity
faiss.normalize_L2(arr)
index = faiss.IndexFlatIP(d)  # inner product on normalized vectors = cosine
index.add(arr)

# keep mapping id -> original doc
id_map = {i: all_data[i] for i in range(len(all_data))}

# 5) query + retrieval
def retrieve(query: str, k=3):
    q_emb = embed_texts([query])[0]
    q = np.array([q_emb]).astype("float32")
    faiss.normalize_L2(q)
    D, I = index.search(q, k)
    hits = [id_map[idx] for idx in I[0] if idx != -1]
    return hits

def print_typing_effect(text, delay=0.03):
    for ch in text:
        print(ch, end="", flush=True)
        time.sleep(delay)

# 6) call LLM with retrieved context
def answer_with_context(user_q: str):
    try:
        hits = retrieve(user_q, k=10)
        if not hits:
            print("Xin l·ªói, m√¨nh ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.")
            return
        
        context = "\n\n".join(
            [
                (
                    f"Source: {h['Source']}\nText: {h['text']}\nType: {h.get('Type','')}"
                    if h.get("is_law")
                    else f"Dialect: {h['dialect']}\nText: {h['text']}\nMeaning: {h.get('meaning','')}"
                )
                for h in hits
            ]
        )

        user_prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam.
        Ng∆∞·ªùi d√πng h·ªèi: {user_q}

        Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p (l·∫•y t·ª´ c∆° s·ªü d·ªØ li·ªáu, ƒë√¢y l√† ngu·ªìn th√¥ng tin gi√∫p b·∫°n tr·∫£ l·ªùi c√¢u h·ªèi):
        {context}

        Y√™u c·∫ßu:
        
        2. N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ t√≠n ƒë√∫ng ƒë·∫Øn hay sai tr√°i c·ªßa lu·∫≠t th√¨ ƒë∆∞a ra c√°c c√°ch ki·ªÉm tra cho ng∆∞·ªùi d√πng. Kh√¥ng tr·∫£ l·ªùi 1 c√°ch ch·∫Øc ch·∫Øn
        3. N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ n·ªôi dung lu·∫≠t ph√°p v·ªÅ v·∫•n ƒë·ªÅ g√¨ th√¨ s·ª≠ d·ª•ng d·ªØ li·ªáu trong Ng·ªØ c·∫£nh. Kh√¥ng s·ª≠a ƒë·ªïi hay b·ªï sung th√™m
        4. Kh√¥ng th√™m b·∫•t k√¨ l∆∞u √Ω hay nh·∫Øc nh·ªù v·ªÅ n·ªôi dung trong Ng·ªØ c·∫£nh
        5. N·∫øu kh√¥ng th·∫•y d·ªØ li·ªáu trong Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p th√¨ tr·∫£ l·ªùi l√† "M·∫∑c d√π d·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c cung c·∫•p theo t√¥i bi·∫øt..." r·ªìi th√™m √Ω c·ªßa b·∫°n v√†o
        6. ƒê·ª´ng nh·∫Øc ƒë·∫øn "Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p" m√† t√¥i ƒë√£ g·ª≠i cho b·∫°n trong c√¢u tr·∫£ l·ªùi
        "
        """
        user_prompt = truncate_prompt(user_prompt)

        # print(user_prompt)

        stream = client.models.generate_content_stream(
            model="gemini-2.5-flash",
            contents=user_prompt
        )
        response = ""

        for chunk in stream:
            response += chunk.text

        return response
    except Exception as e:
        print(f"L·ªói khi g·ªçi API: {e}")
        return "Xin l·ªói, m√¨nh g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n."

# run example
# if __name__ == "__main__":
#     while True:
#         q = input("B·∫°n: ")
#         if q.strip().lower() in ("exit","quit"):
#             break
#         time.sleep(1)
#         print("Bot: ", end="")
#         print_typing_effect(answer_with_context(q))
#         print("\n")

# with gr.Blocks() as demo:
#     gr.Markdown("# ü§ñ")
#     chatbot = gr.Chatbot()
#     msg = gr.Textbox(placeholder="Nh·∫≠p c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t...")
#     clear = gr.Button("X√≥a h·ªôi tho·∫°i")

#     def user_message(user_input, chat_history):
#         response = answer_with_context(user_input)
#         chat_history.append((user_input, response))
#         return "", chat_history

#     msg.submit(user_message, [msg, chatbot], [msg, chatbot])
#     clear.click(lambda: [], None, chatbot, queue=False)

# demo.launch()